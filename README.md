## Introduction

Hi, these are highly opinionated notes I recently started taking after reading some new papers â€” to answer questions that arose while reading each one, as well as random ML questions that popped into my head and the answers I found for them.
I hope future me - or someone else - finds it useful.

### Diffusion Transformers with Representation Autoencoders

I've been wondering if it's possible to run diffusion on a standard autoencoder without the variational baggage, and then this paper dropped: [https://arxiv.org/pdf/2510.11690](https://arxiv.org/pdf/2510.11690)

The authors introduce an approach called RAE (Representation Autoencoder). The main challenges they tackled were the discrete nature of standard AE latent spaces (which lack the continuity needed for diffusion) and the high dimensionality of latents from pretrained encoders.

Interestingly, the authors demonstrate that having a highly "semantic" space isn't a bug, it's a feature. It actually improves generation and accelerates convergence.

Here's the recipe they used to make it work:

1.  They took a pretrained encoder (e.g., DINOv2) and froze it.
2.  They trained a decoder on top of the frozen encoder, but injected Gaussian noise into the latents during training. This noise augmentation makes the decoder robust to the continuous, noisy distribution generated by the diffusion model, effectively side-stepping the need for the VAE formulation.
3.  They trained a DiT with flow matching on these latents. To handle the high-dimensional tokens, they had to:
    - Increase the model width significantly compared to VAE-based diffusion models.
    - Use a specialized noise scheduler that depends on the data dimensionality.

The authors claim a new SOTA (FID 1.13 on ImageNet 512x512) and massive convergence speedups (16-47x faster than VAE counterparts). However, the most interesting takeaway from a research perspective is the validation of the hypothesis that diffusion can be trained efficiently in high-dimensional, semantic spaces without relying on variational methods.
